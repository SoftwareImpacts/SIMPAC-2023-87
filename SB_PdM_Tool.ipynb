{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SB-PdM: a tool for predictive maintenance of rolling bearings based on limited labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.matlib\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pywt\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for Fast Fourier transform (FFT)\n",
    "\n",
    "#Inputs:\n",
    "# x: input signal\n",
    "# fs: samplin freq. \n",
    "# num_samples = length of input signal\n",
    "\n",
    "The function returns:\n",
    "#f: freq. contents\n",
    "#freq_values: freq. spacing\n",
    "\"\"\"\n",
    "\n",
    "def apply_fft(x, fs, num_samples):\n",
    "    f = np.linspace(0.0, (fs/2.0), num_samples//2)\n",
    "    freq_values = fft(x)\n",
    "    freq_values = 2.0/num_samples * np.abs(freq_values[0:num_samples//2])\n",
    "    return f, freq_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function for The short-time Fourier transform (STFT)\n",
    "\n",
    "#Inputs:\n",
    "# x: input signal\n",
    "# fs: samplin freq. \n",
    "# window: type of window to use\n",
    "# seg_length: Length of STFT segment\n",
    "# num_overlaps:  Number of points from input x to overlap between segments\n",
    "# fft_length:Length of FFT \n",
    "\n",
    "The function returns:\n",
    "f: array of sample frequencies.\n",
    "t:array of segment times.\n",
    "abs(s) = Spectrogram \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def apply_stft(x,fs, win, seg_length, num_overlaps, fft_length):\n",
    "    f, t, s = signal.stft(x, fs, window= win, nperseg= seg_length, noverlap= num_overlaps, nfft= fft_length )\n",
    "    return f, t, abs(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to extract WPT-enrpy features from vibrations segments:\n",
    "1- Decompose vibration segment to "num_levels" levels using wavelet packet transform (WPT) and obtain wavelet coefficients.\n",
    "2- Reconstruct elementary waveforms using the obtained coefficients\n",
    "3- Obtain Shannon entropy for each waveform ==> size of entropy features vector = 1 x 2^num_levels\n",
    "\n",
    "#Inputs:\n",
    "x: input signal\n",
    "num_levels: number of WPT decomposition levels\n",
    "wavelet_function: wavelet base function\n",
    "\n",
    "The function returns:\n",
    "wc_entropy_features: entropy features vector of size = 1 x 2^num_levels\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def wpt_entropy(x, num_levels, wavelet_function):\n",
    "    num_features = 2**num_levels\n",
    "    wc_entropy_features = []\n",
    "    #1- Decompose vibration segment to num_levels using wavelet packet transform (WPT) and obtain wavelet coefficients.\n",
    "    wp = pywt.WaveletPacket(x, wavelet = wavelet_function, maxlevel = num_levels) # Wavelet packet transformation\n",
    "    packet_names = [node.path for node in wp.get_level(num_levels, \"natural\")]\n",
    "    \n",
    "    #2- Reconstruct elementary waveforms using the obtained coefficients\n",
    "    for i in range(num_features):\n",
    "        new_wp = pywt.WaveletPacket(data = None, wavelet = wavelet_function, maxlevel = num_levels)\n",
    "        new_wp[packet_names[i]] = wp[packet_names[i]].data\n",
    "        reconstructed_signal = new_wp.reconstruct(update = False) # Signal reconstruction from wavelet packet coefficients\n",
    "        \n",
    "        #3- Obtain Shannon entropy for each waveform ==> size of entropy features vector  = 1 x 2^num_levels\n",
    "        wc_entropy_features.append( -np.nansum(reconstructed_signal**2 * np.log(reconstructed_signal**2)) )# Entropy of reconstructed signal for every node\n",
    "    \n",
    "    return asarray(wc_entropy_features, dtype=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to perform similarity measures:\n",
    "It applies similarity measures between reference samples and test samples.\n",
    "The similarity measures are: Euclidean distance and cosine similarity.\n",
    "\n",
    "#Inputs: \n",
    "baselines: features of labeled reference vibration segments\n",
    "test_data: features of test vibration segments\n",
    "baseline_labels: Labels of reference vibration segments\n",
    "\n",
    "The function returns:\n",
    "ssim_s: SSIM scores of test vibration segments \n",
    "cos_s:  Cosine similarity scores of test vibration segments \n",
    "euc_s:  Euclidean distance scores of test vibration segments\n",
    "y_ssim: Predicted classes using cosine similarity scores of vibrations segments \n",
    "y_cos:  Predicted classes using SSIM scores of vibrations segments \n",
    "y_euc:  Predicted classes using Euclidean distance scores of vibrations segments\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def similarity_measures(baselines,test_data, baseline_labels):\n",
    "    \n",
    "    ssim_s = np.empty(len(test_data)) # SSIM scores of test smaples\n",
    "    cos_s = np.empty(len(test_data)) # Cosine similarity  scores of test smaples \n",
    "    euc_s = np.empty(len(test_data)) # Euclidean distance similarity  scores of test smaples\n",
    "    \n",
    "    y_ssim = np.empty(len(test_data))# Predicted classes using ssim  scores of test smaples\n",
    "    y_cos = np.empty(len(test_data)) # Predicted classes using cosine similarity  scores of test smaples \n",
    "    y_euc = np.empty(len(test_data)) # Predicted classes using Euclidean distance\n",
    " \n",
    "    for i in range(len(test_data)):\n",
    "        s = test_data[i]\n",
    "        ssim_record = np.empty(len(baselines))\n",
    "        dist_record = np.empty(len(baselines))\n",
    "        dist2_record = np.empty(len(baselines))\n",
    "        for k in range(len(baselines)):\n",
    "            ssim_record[k]= ssim(baselines[k], test_data[i])\n",
    "            dist_record[k] = distance.cosine(baselines[k].reshape(-1), test_data[i].reshape(-1)) # Cosine similaritiy\n",
    "            dist2_record[k] = np.mean( sum(abs(s1-s2)**2 for s1, s2 in zip( baselines[k], test_data[i] ))**(1/2) ) # Ecludian distance\n",
    "            \n",
    "        ssim_s[i] = baseline_labels[int(np.argmax(ssim_record))]\n",
    "        cos_s[i] = baseline_labels[int(np.argmin(dist_record))]\n",
    "        euc_s[i] = baseline_labels[int(np.argmin(dist2_record))]\n",
    "        y_ssim[i] = np.max(ssim_s) \n",
    "        y_cos[i] = np.min(cos_s) \n",
    "        y_euc[i] = np.min(euc_s)\n",
    "        \n",
    "    return ssim_s, cos_s, euc_s, y_ssim, y_cos, y_euc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perforem similarity-based_classification:\n",
    "\n",
    "\"\"\"\n",
    "#Inputs: \n",
    "baselines_features: features of labeled reference vibration segments.\n",
    "test_samples_features: features of test vibration segments\n",
    "baseline_labels: Labels of reference vibration segments\n",
    "baselines_avg: True / False ==>  If True, features of reference samples belong the same class will be averaged. \n",
    "This will reduce computitional complexity.\n",
    "\n",
    "The function returns:\n",
    "ssim_s: SSIM scores of test vibration segments \n",
    "cos_s:  Cosine similarity scores of test vibration segments \n",
    "euc_s: Euclidean distance scores of test vibration segments\n",
    "y_cos:  Predicted classes using cosine similarity scores of vibrations segments \n",
    "y_cos:  Predicted classes using cosine similarity scores of vibrations segments \n",
    "y_ssim: Predicted classes using SSIM scores\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def similarity_based_classification(baselines_features, baselines_labels, test_samples_features, baselines_avg = True ):\n",
    "    \n",
    "    if baselines_avg:\n",
    "        # Averaging reference samples based on thier classes; since each class involves 4 different motor speeds. Refers to the paper for more details.\n",
    "        # Averaging reference samples based on thier classes;\n",
    "        baselines_df = pd.DataFrame(data= baselines_features.reshape(len(baselines_features),-1) )\n",
    "        baselines_df['labels']= baselines_labels\n",
    "        baselines_features = baselines_df.groupby('labels').mean().to_numpy()\n",
    "        baselines_labels= np.unique( baselines_labels, axis= 0)\n",
    "\n",
    "        if len(test_samples_features.shape) == 3:  # Reshape STFT features:\n",
    "            w, h = test_samples_features[0].shape\n",
    "            baselines_features = baselines_features.reshape(len(baselines_labels), w, h )\n",
    "               \n",
    "    \n",
    "    print(\"Shape of test samples features shape:\", test_samples_features.shape)\n",
    "    print(\"Shape of reference samples features shape:\", baselines_features.shape)\n",
    "    \n",
    "    # Applying similarity measures:\n",
    "    y_pred_ssim, y_pred_cos, y_pred_Ecl, ssim_score, cos_score, Ecl_score = similarity_measures(baselines_features, \n",
    "                                                                                               test_samples_features, baselines_labels)\n",
    "    return y_pred_ssim, y_pred_cos, y_pred_Ecl, ssim_score, cos_score, Ecl_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================================\n",
    "# Loading Data:\n",
    "\n",
    "## Note:\n",
    "\n",
    "### The vibratinon samples are extracted CWRU dataset. Link to  access the dataset: https://engineering.case.edu/bearingdatacenter\n",
    "\n",
    "=========================================================================================================================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test vibration samples: (2979, 2000)\n",
      "Size of labeled reference vibration samples: (40, 2000)\n",
      "Size of test & reference vibration samples: (3019, 2000)\n",
      "Time duration of each vibration segment =  0.16666666666666666 seconds.\n"
     ]
    }
   ],
   "source": [
    "## Load Test samples and thier labels from CSV file:\n",
    "df = pd.read_csv(\"./Test_Samples.csv\")\n",
    "y_test = df['labels'].to_numpy()\n",
    "test_data = df.drop(['labels'], axis=1).to_numpy()\n",
    "\n",
    "print(\"Size of test vibration samples:\", test_data.shape)\n",
    "\n",
    "## Load Reference samples and thier labels from CSV file:\n",
    "df = pd.read_csv(\"./Reference_Samples.csv\")\n",
    "baselines_labels = df['labels'].to_numpy()\n",
    "baselines = df.drop(['labels'], axis=1).to_numpy()\n",
    "print(\"Size of labeled reference vibration samples:\", baselines.shape)\n",
    "\n",
    "X = np.concatenate( (baselines, test_data) , axis=0, out=None) # Nummpy array contians test [samples:reference labels] for feature extraction\n",
    "print(\"Size of test & reference vibration samples:\", X.shape)\n",
    "\n",
    "baselines_num = len(baselines)\n",
    "\n",
    "\n",
    "fs = 12000 # Sample rate of vibration data.\n",
    "num_samples = X.shape[1] # number of datapoints in each vibration segment\n",
    "print(\"Time duration of each vibration segment = \", num_samples /fs, \"seconds.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================================\n",
    "# Feature Extraction:\n",
    "=========================================================================================================================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFT features shape: (3019, 1000)\n",
      "STFT features shape: (3019, 515, 41)\n",
      "WPT-Entropy features shape: (3019, 32)\n"
     ]
    }
   ],
   "source": [
    "# Note: Here, a noise-free scenario is considered.\n",
    "\n",
    "# Features extraction:\n",
    "fft_z = []\n",
    "stft_z = []\n",
    "wpt = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    #FFT\n",
    "    f, z = apply_fft(X[i], fs, num_samples)\n",
    "    fft_z.append(z)\n",
    "    \n",
    "    #STFT\n",
    "    f, t, z = apply_stft(X[i], fs, 'hamming', (num_samples/2), int( 0.95*(num_samples/2) ), 1028)\n",
    "    stft_z.append(z)\n",
    "    \n",
    "    # WPT entropy\n",
    "    w = wpt_entropy(X[i], 5, \"db4\")\n",
    "    wpt.append(w)\n",
    "    \n",
    "fft_features = np.asarray(fft_z, dtype=\"float\")\n",
    "print('FFT features shape:', fft_features.shape)\n",
    "stft_features = np.asarray(stft_z, dtype=\"float\")\n",
    "print('STFT features shape:', stft_features.shape)\n",
    "wpt_features = np.asarray(wpt, dtype=\"float\")\n",
    "print('WPT-Entropy features shape:', wpt_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================================\n",
    "# Similarity-Based Classification:\n",
    "=========================================================================================================================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples size; (40, 1000)\n",
      "Reference samples size (2979, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Select feature type: FFT features, STFT features, or WPT-entropy features\n",
    "features =  fft_features # ====>  fft_features / stft_features / wpt_features\n",
    "\n",
    "# Split features into reference and test samples:\n",
    "baselines_features = features[:baselines_num]\n",
    "print(\"Test samples size;\", baselines_features.shape)\n",
    "test_samples_features = features[baselines_num:]\n",
    "print(\"Reference samples size\", test_samples_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test samples features shape: (2979, 1000)\n",
      "Shape of reference samples features shape: (10, 1000)\n"
     ]
    }
   ],
   "source": [
    "y_pred_ssim, y_pred_cos, y_pred_Ecl, ssim_score, cos_score, Ecl_score = similarity_based_classification(baselines_features,\n",
    "                                                                        baselines_labels, test_samples_features, \n",
    "                                                                        baselines_avg = True    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================================\n",
    "# Performance Metrics:\n",
    "=========================================================================================================================== "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity-Accuracy: 99.59718026183283 %\n",
      "Cosine similarity-Confusion Matrix:\n",
      "[[842   0   0   0   0   0   0   0   0   0]\n",
      " [  0 238   0   0   0   0   0   0   0   0]\n",
      " [  0   0 228   0   0   0   0   0   0   8]\n",
      " [  0   0   0 237   0   0   0   0   0   0]\n",
      " [  0   0   0   0 237   0   0   0   0   0]\n",
      " [  0   0   0   0   0 238   0   0   0   0]\n",
      " [  0   0   0   0   4   0 234   0   0   0]\n",
      " [  0   0   0   0   0   0   0 238   0   0]\n",
      " [  0   0   0   0   0   0   0   0 237   0]\n",
      " [  0   0   0   0   0   0   0   0   0 238]]\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "y_predicted = y_pred_cos\n",
    "accuracy =accuracy_score(y_test, y_predicted) \n",
    "conf_mat =confusion_matrix(y_test, y_predicted)\n",
    "print(\"Cosine similarity-Accuracy:\", accuracy*100, \"%\")\n",
    "print(\"Cosine similarity-Confusion Matrix:\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance-Accuracy: 99.36220208123532 %\n",
      "Euclidean distance-Confusion Matrix:\n",
      "[[842   0   0   0   0   0   0   0   0   0]\n",
      " [  0 238   0   0   0   0   0   0   0   0]\n",
      " [  0   0 236   0   0   0   0   0   0   0]\n",
      " [  0   0   0 237   0   0   0   0   0   0]\n",
      " [  0   0   0   0 236   0   1   0   0   0]\n",
      " [  0   0   0   0   0 229   1   0   8   0]\n",
      " [  0   0   0   0   7   2 229   0   0   0]\n",
      " [  0   0   0   0   0   0   0 238   0   0]\n",
      " [  0   0   0   0   0   0   0   0 237   0]\n",
      " [  0   0   0   0   0   0   0   0   0 238]]\n"
     ]
    }
   ],
   "source": [
    "# Euclidean distance\n",
    "y_predicted = y_pred_Ecl\n",
    "accuracy =accuracy_score(y_test, y_predicted) \n",
    "conf_mat =confusion_matrix(y_test, y_predicted)\n",
    "print(\"Euclidean distance-Accuracy:\", accuracy*100, \"%\")\n",
    "print(\"Euclidean distance-Confusion Matrix:\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM-Accuracy: 99.39577039274926 %\n",
      "SSIM-Confusion Matrix:\n",
      "[[842   0   0   0   0   0   0   0   0   0]\n",
      " [  0 238   0   0   0   0   0   0   0   0]\n",
      " [  0   0 236   0   0   0   0   0   0   0]\n",
      " [  0   0   0 237   0   0   0   0   0   0]\n",
      " [  0   0   0   0 237   0   0   0   0   0]\n",
      " [  0   0   0   0   0 231   5   0   2   0]\n",
      " [  0   0   0   0  11   0 227   0   0   0]\n",
      " [  0   0   0   0   0   0   0 238   0   0]\n",
      " [  0   0   0   0   0   0   0   0 237   0]\n",
      " [  0   0   0   0   0   0   0   0   0 238]]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = y_pred_ssim\n",
    "accuracy =accuracy_score(y_test, y_predicted) \n",
    "conf_mat =confusion_matrix(y_test, y_predicted)\n",
    "print(\"SSIM-Accuracy:\", accuracy*100, \"%\")\n",
    "print(\"SSIM-Confusion Matrix:\")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
