{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#     Code to segment vibration signals and create a labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.io import loadmat\n",
    "from numpy import asarray\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @@@ Function to create dataset from raw vibration signals @@@\n",
    "# The function processes .mat vibration data files and do segmentation of vibration signals according to \n",
    "# desired length of the input segment \"num_samples\".\n",
    "\n",
    "# @@@@ Inputs: \n",
    "#data_src: path to dataset folders\n",
    "#req_Key: key to extract vibration data from .mat files\n",
    "#num_samples:  required length of input vibration samples \"number of data points per sample\".\n",
    "#class_: label/class of vibration data\n",
    "\n",
    "# @@@@ The function returns:\n",
    "# reference vibration samples \"baselines\" and  thier labels \"baselines_labels\"\n",
    "# test vibration samples \"segmented_data\" and  thier labels \"labels\"\n",
    "\n",
    "def make_dataset(data_src, req_Key, num_samples, class_):\n",
    "    all_num_segments = []\n",
    "    pattern = re.compile(req_Key)\n",
    "    files = glob.glob(data_src)\n",
    "    files = np.sort(files)\n",
    "    for file in files:\n",
    "        data = loadmat(file)\n",
    "        keysList = [key for key in data]\n",
    "        for key in keysList:\n",
    "            if pattern.search(key):\n",
    "                my_key = key\n",
    "                drive_end_data = data[my_key] \n",
    "                num_segments = np.floor(len(drive_end_data)/num_samples)\n",
    "                all_num_segments.append(num_segments)\n",
    "                total_num_segments = sum(all_num_segments)\n",
    "    L = len(all_num_segments)\n",
    "    segmented_data = np.empty([int(total_num_segments)-L, num_samples])\n",
    "    baselines = np.empty([L,num_samples])\n",
    "    num = 0\n",
    "    k = 0\n",
    "    for file in files:\n",
    "        data = loadmat(file)\n",
    "        keysList = [key for key in data]\n",
    "        for key in keysList:\n",
    "            if pattern.search(key):\n",
    "                my_key = key\n",
    "                drive_end_data = data[my_key] \n",
    "                num_segments = np.floor(len(drive_end_data)/num_samples)\n",
    "                for i in range(int(num_segments)):\n",
    "                    if i == 0:\n",
    "                        baselines[k,:] = drive_end_data[i*num_samples:(i+1)*num_samples, 0]\n",
    "                        k = k +1\n",
    "                    else:\n",
    "                        segmented_data[num,:] = drive_end_data[i*num_samples:(i+1)*num_samples, 0]\n",
    "                        num = num + 1\n",
    "    segmented_data = np.unique(segmented_data, axis= 0)# Remove duplicates\n",
    "    baselines = np.unique(baselines, axis= 0)# Remove duplicates\n",
    "    labels = np.ones(len(segmented_data))*class_\n",
    "    baselines_labels = np.ones(len(baselines))*class_\n",
    "    return segmented_data, labels, baselines, baselines_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================================================\n",
    "# Dataset Creatioin:\n",
    "=========================================================================================================================== "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping vibration signals in required folders to  generate datasets\n",
    "Link to download CWRU dataset: https://engineering.case.edu/bearingdatacenter <br>\n",
    "After downloading the dataset, vibration signals to be grouped according to their operational conditions/properties* in 10 folders (corresponding to number of operational classes in the dataset: 1 normal class and 9 faulty classes) with below folder names:\n",
    "\n",
    "1.\t12K_DE_Normal \n",
    "2.\t12k_DE_IRFault_0.007\n",
    "3.\t12k_DE_IRFault_0.014\n",
    "4.\t12k_DE_IRFault_0.021\n",
    "5.\t12k_DE_BallFault_0.007\n",
    "6.\t12k_DE_BallFault_0.014\n",
    "7.\t12k_DE_BallFault_0.021\n",
    "8.\t12k_DE_ORFault_0.007\n",
    "9.\t12k_DE_ORFault_0.014\n",
    "10.\t12k_DE_ORFault_0.021\n",
    "\n",
    "*12k = sampling rate of vibration signals <br>\n",
    "*DE= Drive End vibration data <br>\n",
    "*IRFault = Inner Race faults <br>\n",
    "*BallFault = Ball faults <br>\n",
    "*ORFault = Outer Race faults <br>\n",
    "*0.014/0.021/0.007= fault diameter <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 12000; # Sampling rate ( sample/second) of data collection\n",
    "num_samples = 2000 # Set required length of input vibration samples \"number of data points per sample\".\n",
    "Key = \"_DE_time\" # key to extract  Drive End (DE) vibration data from .mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1 features shape: (842, 2000)\n",
      "Class 1 labels shape: (842,)\n",
      "Class 2 features shape: (238, 2000)\n",
      "Class 2 labels shape: (238,)\n",
      "Class 3 features shape: (236, 2000)\n",
      "Class 3 labels shape: (236,)\n",
      "Class 4 features shape: (237, 2000)\n",
      "Class 4 labels shape: (237,)\n",
      "Class 5 features shape: (237, 2000)\n",
      "Class 5 labels shape: (237,)\n",
      "Class 6 features shape: (238, 2000)\n",
      "Class 6 labels shape: (238,)\n",
      "Class 7 features shape: (238, 2000)\n",
      "Class 7 labels shape: (238,)\n",
      "Class 8 features shape: (238, 2000)\n",
      "Class 8 labels shape: (238,)\n",
      "Class 9 features shape: (237, 2000)\n",
      "Class 9 labels shape: (237,)\n",
      "Class 10 features shape: (238, 2000)\n",
      "Class 10 labels shape: (238,)\n"
     ]
    }
   ],
   "source": [
    "data_path = (r\".\\dataset\") # path were the folders contain vibration signals are located.\n",
    "\n",
    "cls_1 = '12K_DE_Normal/*'; cls_2 = '12k_DE_IRFault_0.007/*'; cls_3 = '12k_DE_IRFault_0.014/*'; \n",
    "cls_4 = '12k_DE_IRFault_0.021/*'; cls_5 = '12k_DE_BallFault_0.007/*'\n",
    "cls_6 = '12k_DE_BallFault_0.014/*'; cls_7 = '12k_DE_BallFault_0.021/*'\n",
    "cls_8 = '12k_DE_ORFault_0.007/*'; cls_9 = '12k_DE_ORFault_0.014/*'; cls_10 ='12k_DE_ORFault_0.021/*'\n",
    "\n",
    "norm, y_norm, norm_baselines, y_norm_baseline = make_dataset(os.path.join(data_path, cls_1), Key, num_samples, 1)\n",
    "defc1, y_defc1, defc1_baslines, y_defc1_baseline  = make_dataset(os.path.join(data_path, cls_2), Key, num_samples, 2)\n",
    "defc2, y_defc2, defc2_baslines, y_defc2_baseline = make_dataset(os.path.join(data_path, cls_3), Key, num_samples, 3)\n",
    "defc3, y_defc3, defc3_baslines, y_defc3_baseline = make_dataset(os.path.join(data_path, cls_4), Key, num_samples, 4)\n",
    "defc4, y_defc4, defc4_baslines, y_defc4_baseline = make_dataset(os.path.join(data_path, cls_5), Key, num_samples, 5)\n",
    "defc5, y_defc5, defc5_baslines, y_defc5_baseline = make_dataset(os.path.join(data_path, cls_6), Key, num_samples, 6)\n",
    "defc6, y_defc6, defc6_baslines, y_defc6_baseline = make_dataset(os.path.join(data_path, cls_7), Key, num_samples, 7)\n",
    "defc7, y_defc7, defc7_baslines, y_defc7_baseline = make_dataset(os.path.join(data_path, cls_8), Key, num_samples, 8)\n",
    "defc8, y_defc8, defc8_baslines, y_defc8_baseline = make_dataset(os.path.join(data_path, cls_9), Key, num_samples, 9)\n",
    "defc9, y_defc9, defc9_baslines, y_defc9_baseline = make_dataset(os.path.join(data_path, cls_10), Key, num_samples, 10)\n",
    "print('Class 1 features shape:', norm.shape); print('Class 1 labels shape:', y_norm.shape)\n",
    "print('Class 2 features shape:', defc1.shape); print('Class 2 labels shape:', y_defc1.shape)\n",
    "print('Class 3 features shape:', defc2.shape); print('Class 3 labels shape:', y_defc2.shape)\n",
    "print('Class 4 features shape:', defc3.shape); print('Class 4 labels shape:', y_defc3.shape)\n",
    "print('Class 5 features shape:', defc4.shape); print('Class 5 labels shape:', y_defc4.shape)\n",
    "print('Class 6 features shape:', defc5.shape); print('Class 6 labels shape:', y_defc5.shape)\n",
    "print('Class 7 features shape:', defc6.shape); print('Class 7 labels shape:', y_defc6.shape)\n",
    "print('Class 8 features shape:', defc7.shape); print('Class 8 labels shape:', y_defc7.shape)\n",
    "print('Class 9 features shape:', defc8.shape); print('Class 9 labels shape:', y_defc8.shape)\n",
    "print('Class 10 features shape:', defc9.shape); print('Class 10 labels shape:', y_defc9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples shape: (2979, 2000)\n",
      "Baselines samples shape: (40, 2000)\n"
     ]
    }
   ],
   "source": [
    "##Test Samples:\n",
    "\n",
    "test_data = np.concatenate( (norm, defc1, defc2, defc3, defc4, defc5, defc6, defc7, defc8,  defc9 ) , axis=0, out=None)\n",
    "y_test = np.concatenate( (y_norm, y_defc1, y_defc2, y_defc3, y_defc4, y_defc5, \n",
    "                          y_defc6, y_defc7, y_defc8, y_defc9  ), axis=0, out=None)\n",
    "#Shuffle test samples\n",
    "df = pd.DataFrame(data=test_data)\n",
    "df['labels'] = y_test\n",
    "df = df.sample(frac = 1)\n",
    "y_test = df['labels'].to_numpy()\n",
    "\n",
    "test_data = df.drop(['labels'], axis=1).to_numpy()\n",
    "\n",
    "##### Save Test samples to a CSV file:#############\n",
    "df.to_csv(r'.\\Test_Samples.csv', index=False)\n",
    "##################################################\n",
    "print(\"Test samples shape:\", test_data.shape )\n",
    "\n",
    "\n",
    "## Reference Samples\n",
    "baselines = np.concatenate( (norm_baselines, defc1_baslines, defc2_baslines, defc3_baslines, defc4_baslines, defc5_baslines, \n",
    "                     defc6_baslines, defc7_baslines, defc8_baslines,  defc9_baslines ) , axis=0, out=None)\n",
    "baselines_labels = np.concatenate( (y_norm_baseline, y_defc1_baseline, y_defc2_baseline, y_defc3_baseline, \n",
    "                                    y_defc4_baseline, y_defc5_baseline, y_defc6_baseline, y_defc7_baseline,\n",
    "                                    y_defc8_baseline, y_defc9_baseline), axis=0, out=None)\n",
    "\n",
    "##### Save Test samples to a CSV file:###############\n",
    "df = pd.DataFrame(data=baselines)\n",
    "df['labels'] = baselines_labels\n",
    "df.to_csv(r'.\\Reference_Samples.csv', index=False)\n",
    "#####################################################\n",
    "print(\"Baselines samples shape:\", baselines.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
